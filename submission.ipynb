{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Pandas untuk manipulasi dan analisis data\n",
    "pd.options.mode.chained_assignment = None  # Menonaktifkan peringatan chaining\n",
    "\n",
    "\n",
    "app_reviews = pd.read_csv(\"reviews.csv\")\n",
    "\n",
    "df_app_reviews = pd.DataFrame(app_reviews)\n",
    "\n",
    "review_counter, column_counter = df_app_reviews.shape\n",
    "\n",
    "\n",
    "print(review_counter)\n",
    "print(column_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_app_reviews = df_app_reviews.drop_duplicates()\n",
    "clean_df_app_reviews = clean_df_app_reviews.drop(columns=[\"reviewId\",\"userName\",\"userImage\",\"score\",\"thumbsUpCount\",\"reviewCreatedVersion\",\"at\",\"replyContent\",\"repliedAt\",\"appVersion\"])\n",
    "\n",
    "clean_review_counter, clean_column_counter = clean_df_app_reviews.shape\n",
    "\n",
    "clean_df_app_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sastrawi nltk wordcloud\n",
    "\n",
    "import nltk  # Import pustaka NLTK (Natural Language Toolkit).\n",
    "nltk.download('punkt_tab')  # Mengunduh dataset yang diperlukan untuk tokenisasi teks.\n",
    "nltk.download('stopwords')  # Mengunduh dataset yang berisi daftar kata-kata berhenti (stopwords) dalam berbagai bahasa.\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    " \n",
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # menghapus RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # menghapus link\n",
    "    text = re.sub(r'[0-9]+', '', text) # menghapus angka\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # menghapus karakter selain huruf dan angka\n",
    " \n",
    "    text = text.replace('\\n', ' ') # mengganti baris baru dengan spasi\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca\n",
    "    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks\n",
    "    return text\n",
    " \n",
    "def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    return text\n",
    " \n",
    "def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    " \n",
    "def filteringText(text): # Menghapus stopwords dalam teks\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',\"di\",\"ga\",\"ya\",\"gaa\",\"loh\",\"kah\",\"woi\",\"woii\",\"woy\"])\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered\n",
    "    return text\n",
    " \n",
    "def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata\n",
    "    # Membuat objek stemmer\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    " \n",
    "    # Memecah teks menjadi daftar kata\n",
    "    words = text.split()\n",
    " \n",
    "    # Menerapkan stemming pada setiap kata dalam daftar\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    " \n",
    "    # Menggabungkan kata-kata yang telah distem\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    " \n",
    "    return stemmed_text\n",
    " \n",
    "def toSentence(list_words): # Mengubah daftar kata menjadi kalimat\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangwords = {\"@\": \"di\", \"abis\": \"habis\", \"wtb\": \"beli\", \"masi\": \"masih\", \"wts\": \"jual\", \"wtt\": \"tukar\", \"bgt\": \"banget\", \"maks\": \"maksimal\",\"gua\": \"saya\",\"bod*h\":\"bodoh\",\"henti\"\"\":\"henti-henti\",\"gada\":\"tidak ada\"}\n",
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    " \n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    " \n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membersihkan teks dan menyimpannya di kolom 'text_clean'\n",
    "clean_df_app_reviews['text_clean'] = clean_df_app_reviews['content'].apply(cleaningText)\n",
    " \n",
    "# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'\n",
    "clean_df_app_reviews['text_casefoldingText'] = clean_df_app_reviews['text_clean'].apply(casefoldingText)\n",
    " \n",
    "# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'\n",
    "clean_df_app_reviews['text_slangwords'] = clean_df_app_reviews['text_casefoldingText'].apply(fix_slangwords)\n",
    " \n",
    "# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'\n",
    "clean_df_app_reviews['text_tokenizingText'] = clean_df_app_reviews['text_slangwords'].apply(tokenizingText)\n",
    " \n",
    "# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'\n",
    "clean_df_app_reviews['text_stopword'] = clean_df_app_reviews['text_tokenizingText'].apply(filteringText)\n",
    " \n",
    "# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'\n",
    "clean_df_app_reviews['text_akhir'] = clean_df_app_reviews['text_stopword'].apply(toSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_app_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    " \n",
    "# Membaca data kamus kata-kata positif dari GitHub\n",
    "lexicon_positive = dict()\n",
    " \n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    " \n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
    " \n",
    "    for row in reader:\n",
    "        # Mengulangi setiap baris dalam file CSV\n",
    "        lexicon_positive[row[0]] = int(row[1])\n",
    "        # Menambahkan kata-kata positif dan skornya ke dalam kamus lexicon_positive\n",
    "else:\n",
    "    print(\"Failed to fetch positive lexicon data\")\n",
    " \n",
    "# Membaca data kamus kata-kata negatif dari GitHub\n",
    "lexicon_negative = dict()\n",
    " \n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    " \n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
    " \n",
    "    for row in reader:\n",
    "        # Mengulangi setiap baris dalam file CSV\n",
    "        lexicon_negative[row[0]] = int(row[1])\n",
    "        # Menambahkan kata-kata negatif dan skornya dalam kamus lexicon_negative\n",
    "else:\n",
    "    print(\"Failed to fetch negative lexicon data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menentukan polaritas sentimen dari tweet\n",
    " \n",
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    #for word in text:\n",
    " \n",
    "    score = 0\n",
    "    # Inisialisasi skor sentimen ke 0\n",
    " \n",
    "    for word in text:\n",
    "        # Mengulangi setiap kata dalam teks\n",
    "        \n",
    "        if (word in lexicon_positive):\n",
    "            score = score + lexicon_positive[word]\n",
    "            # Jika kata ada dalam kamus positif, tambahkan skornya ke skor sentimen\n",
    "        elif (word in lexicon_negative):\n",
    "            score = score + lexicon_negative[word]\n",
    "        else:\n",
    "            score = 0\n",
    " \n",
    "    polarity=''\n",
    "    # Inisialisasi variabel polaritas\n",
    " \n",
    "    if (score > 0):\n",
    "        polarity = 'positive'\n",
    "        # Jika skor sentimen lebih besar atau sama dengan 0, maka polaritas adalah positif\n",
    "    elif (score < 0):\n",
    "        polarity = 'negative'\n",
    "    else:\n",
    "        polarity = 'neutral'\n",
    "        # Jika skor sentimen kurang dari 0, maka polaritas adalah negatif\n",
    "    return score, polarity\n",
    "    # Mengembalikan skor sentimen dan polaritas teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clean_df_app_reviews['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "results = list(zip(*results))\n",
    "\n",
    "clean_df_app_reviews['polarity_score'] = results[0]\n",
    "clean_df_app_reviews['polarity'] = results[1]\n",
    "print(clean_df_app_reviews['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create one hot encoding for column polarity label\n",
    "category = pd.get_dummies(clean_df_app_reviews.polarity)\n",
    "\n",
    "#concat result from one hot encoding\n",
    "new_df_app_reviews = pd.concat([clean_df_app_reviews,category],axis=1)\n",
    "\n",
    "#drop unused column label\n",
    "new_df_app_reviews = new_df_app_reviews.drop(columns=[\"content\",\"text_clean\",\"text_casefoldingText\",\"text_slangwords\",\"text_tokenizingText\",\"text_stopword\",\"polarity_score\"])\n",
    "\n",
    "new_df_app_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = new_df_app_reviews[\"text_akhir\"].values\n",
    "labels = new_df_app_reviews[[\"negative\",\"neutral\",\"positive\"]].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "\n",
    "text_train, text_test, label_train, label_test = train_test_split(text,labels,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000,oov_token=\"x\")\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "tokenizer.fit_on_texts(text_test)\n",
    "\n",
    "\n",
    "sequence_train = tokenizer.texts_to_sequences(text_train)\n",
    "sequence_test = tokenizer.texts_to_sequences(text_test)\n",
    "\n",
    "pad_sequence_train = pad_sequences(sequence_train)\n",
    "pad_sequence_test = pad_sequences(sequence_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "callback_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor = 'val_loss',\n",
    "    factor = 0.2,\n",
    "    patience = 5,\n",
    "    min_lr = 1.5e-5\n",
    ")\n",
    "\n",
    "callback_early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    min_delta = 0,\n",
    "    patience = 12,\n",
    "    verbose = 0,\n",
    "    mode = \"auto\",\n",
    "    baseline = None,\n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as ly # type: ignore\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    ly.Input(shape=(200,),),\n",
    "    ly.Embedding(input_dim=6000,output_dim=64),\n",
    "    ly.Dropout(0.5),\n",
    "    ly.LSTM(64),\n",
    "    ly.Dropout(0.3),\n",
    "    ly.Dense(3,activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    pad_sequence_train,\n",
    "    label_train,\n",
    "    epochs=4,\n",
    "    batch_size=128,\n",
    "    callbacks=[callback_reduce_lr,callback_early_stop],\n",
    "    verbose=1,\n",
    "    validation_data=(pad_sequence_test,label_test)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "\n",
    "plt.plot(epochs,accuracy,'r',label=\"Accuracy Training\")\n",
    "plt.plot(epochs,val_accuracy,'b',label=\"Accuracy Validation\")\n",
    "plt.title('Accuracy training and validation',loc='left',pad=30,fontsize=20,color='red')\n",
    "plt.ylabel('Accuracy',color = 'blue')\n",
    "plt.xlabel('Epoch',color = 'blue')\n",
    "plt.legend(loc=0)\n",
    "plt.grid(color='darkgray',linestyle=':',linewidth=0.5)\n",
    "plt.ylim(ymin=0)\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs,loss,'r',label=\"Loss Training\")\n",
    "plt.plot(epochs,val_loss,'b',label=\"Loss Validation\")\n",
    "plt.title('Loss training and validation',loc='left',pad=30,fontsize=20,color='red')\n",
    "plt.ylabel('Loss',color = 'blue')\n",
    "plt.xlabel('Epoch',color = 'blue')\n",
    "plt.legend(loc=0)\n",
    "plt.grid(color='darkgray',linestyle=':',linewidth=0.5)\n",
    "plt.ylim(ymin=0)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF X RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Pisahkan data menjadi fitur (tweet) dan label (sentimen)\n",
    "X = new_df_app_reviews['text_akhir']\n",
    "y = new_df_app_reviews['polarity']\n",
    " \n",
    "# Ekstraksi fitur dengan TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8 )\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "# Bagi data menjadi data latih dan data uji\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Membuat objek model Random Forest\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Melatih model Random Forest pada data pelatihan\n",
    "random_forest.fit(X_train.toarray(), y_train)\n",
    " \n",
    "# Prediksi sentimen pada data pelatihan dan data uji\n",
    "y_pred_train_rf = random_forest.predict(X_train.toarray())\n",
    "y_pred_test_rf = random_forest.predict(X_test.toarray())\n",
    " \n",
    "# Evaluasi akurasi model Random Forest\n",
    "accuracy_train_rf = accuracy_score(y_pred_train_rf, y_train)\n",
    "accuracy_test_rf = accuracy_score(y_pred_test_rf, y_test)\n",
    " \n",
    "# Menampilkan akurasi\n",
    "print('Random Forest - accuracy_train:', accuracy_train_rf)\n",
    "print('Random Forest - accuracy_test:', accuracy_test_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec X Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim numpy==1.26.0\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "tokenized_data = [word_tokenize(sentence.lower()) for sentence in text]\n",
    "\n",
    "model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Function to get word embeddings for each review\n",
    "def get_review_embedding(review, model):\n",
    "    # Initialize an empty list to store word vectors\n",
    "    word_vectors = []\n",
    "    \n",
    "    for word in review:\n",
    "        if word in model.wv:  # Check if word exists in the Word2Vec model\n",
    "            word_vectors.append(model.wv[word])\n",
    "    \n",
    "    # Average the word vectors to get a single embedding for the review\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        # If no words in the review have embeddings, return a zero vector\n",
    "        return np.zeros(model.vector_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_embeddings = np.array([get_review_embedding(review, model) for review in tokenized_data])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(review_embeddings, labels, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat objek model Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Melatih model Random Forest pada data pelatihan\n",
    "rf.fit(X_train, y_train)\n",
    " \n",
    "# Prediksi sentimen pada data pelatihan dan data uji\n",
    "y_pred_train_rf = rf.predict(X_train)\n",
    "y_pred_test_rf = rf.predict(X_test)\n",
    " \n",
    "# Evaluasi akurasi model Random Forest\n",
    "accuracy_train_rf = accuracy_score(y_pred_train_rf, y_train)\n",
    "accuracy_test_rf = accuracy_score(y_pred_test_rf, y_test)\n",
    " \n",
    "# Menampilkan akurasi\n",
    "print('Random Forest - accuracy_train:', accuracy_train_rf)\n",
    "print('Random Forest - accuracy_test:', accuracy_test_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
